% close all
% clear all
addpath 'cifar-10-batches-mat';
%% Reading data and initialize the parameters of the network
fprintf('   --> Running Code \n'); 
fprintf('GD parameters '); 
GD=GDparams;
GD.n_batch=100;
GD.n_epochs=40;
GD.eta=0.035;
lambda=0.000001;
fprintf('- done\n'); 
%%
fprintf('Loading Batch '); 
[trainX, trainY, trainy] = LoadBatch('data_batch_1');              %Data for training
[valX, valY, valy] = LoadBatch('data_batch_2');                    %Data for validation
[testX, testY, testy] = LoadBatch('test_batch.mat');               %For Testing
fprintf('- done \n'); 
fprintf('Preprocessing data '); 
trainX = reshape(trainX,3072,10000);    %trainX with size dxn  -> 3072x10000
valX = reshape(valX,3072,10000);        %valX with size dxn  -> 3072x10000
testX = reshape(testX,3072,10000);      %testX with size dxn  -> 3072x10000
d=size(trainX,1); 
%More prepocessing
mean_trainX = mean(trainX, 2);
trainX = trainX - repmat(mean_trainX, [1, size(trainX, 2)]);
fprintf('- done\n'); 
fprintf('Running substractions with mean_X '); 
%Subtraction from validated and tested data
valX = valX - repmat(mean_trainX, [1, size(valX, 2)]);
testX = testX - repmat(mean_trainX, [1, size(testX, 2)]);
fprintf('- done\n');
%% Initiating W1, W2, b1 and b2
fprintf('Initialization of W{} and b{} ');
m=50; K=10;
[W,b] = InitParams(d,m,K);
fprintf('- done \n');
%     -------> Termina Ex1
%%    -------> Beg of Ex2
fprintf('Evaluating Classifier ');
[P,h,s1] = EvaluateClassifier(trainX, W, b);
fprintf('- done \n');
%%   This is for the cost function
fprintf('Computing Cost ');
[J,J1] = ComputeCost(trainX,trainY,W,b,lambda);
fprintf('- done \n');
fprintf('      > Initial Loss = %d\n', J);
%% Accuracy
fprintf('Computing Accuracy ');
acc = ComputeAccuracy(trainX,trainY,P);
fprintf('- done \n');
%% Gradients
fprintf('Computing Gradients ');
[LW,Lb,JW,Jb] = ComputeGradients(trainX, trainY, P, W,b, h, s1, lambda);
fprintf('- done \n');

%% Evaluation of gradients

% [LW,Lb,JW,Jb] = ComputeGradients(trainX(:,1), trainY(:,1), P, W,b, h, s1, lambda);
% fprintf('    *Analitical calculated \n');
% 
% h = 1e-5;
% [grad_b, grad_W] = ComputeGradsNum(trainX(:,1), trainY(:,1), W,b, lambda, h);
% fprintf('    *Numerical calculated \n');
% fprintf('Numerical-Analitical comparison done\n');

%% WOrking with minibatches   -- This one is using one part of the data

%  [W1star,W2star,b1star,b2star,JK] = MiniBatchGD(trainX(:,1:700), trainY(:,1:700), GD, cell2mat(W1),cell2mat(W2),cell2mat(b1),cell2mat(b2), lambda);
%  fprintf(' minibatch done \n');
%  figure;
%  plot(JK);
% [P,h,s1] = EvaluateClassifier(trainX(:,1:700), W1star, W2star, b1star, b2star);
% accN = ComputeAccuracy(trainX(:,1:700),trainY(:,1:700),P);
% fprintf('Initial Loss = %d\n', J);

%% Working with the full batch of data
fprintf('Using MiniBatch ');
tic
[Wstar,bstar,JK] = MiniBatchGD(trainX, trainY, GD, W, b, lambda);
toc
fprintf('- done \n');
figure
plot(JK)
[Pn,h,s1] = EvaluateClassifier(trainX, Wstar, bstar);
acc_New = ComputeAccuracy(trainX,trainY,Pn);
fprintf('    > New Accuracy = %d\n', acc_New);
title(strcat('n batch: ',num2str(GD.n_batch),' epochs: ',num2str(GD.n_epochs),' eta: ',num2str(GD.eta),' lambda: ',num2str(lambda)));
%% For Validation Data
fprintf('Computing for Validated Data ');
% [P_val,h_val,s1_val] = EvaluateClassifier(valX, W, b);
% [J_val,J1_val] = ComputeCost(valX,valY,W,b,lambda);
% acc_val = ComputeAccuracy(valX,valY,P_val);
%[LW_val,Lb_val,JW_val,Jb_val] = ComputeGradients(valX, valY, P, W,b, h, s1, lambda);
[Wstar_val,bstar_val,JK_val] = MiniBatchGD(valX, valY, GD, W, b, lambda);
fprintf('- done\n');
figure
[P_val,h_val,s1_val] = EvaluateClassifier(valX, Wstar, bstar);
acc_New_val = ComputeAccuracy(valX,valY,P_val);
fprintf('Accuracy for Validated Data= %d\n', acc_New_val);
%%
plot(0:GD.n_epochs,JK,'r',0:GD.n_epochs,JK_val,'b');
legend('training loss','validation loss');
title(strcat('n batch: ',num2str(GD.n_batch),' epochs: ',num2str(GD.n_epochs),' eta: ',num2str(GD.eta),' lambda: ',num2str(lambda)));


%%
% Breteando bien, 82% y decreciendo de lo mas bien
% GD=GDparams;
% GD.n_batch=100;
% GD.n_epochs=200;
% GD.eta=0.1;
% lambda=0;
% 
%  using small amount of data: acc16%
% GD=GDparams;
% GD.n_batch=100;
% GD.n_epochs=200;
% GD.eta=0.01;
% lambda=0;
%  quitando las lineas de momentum
% 
% 
% [P_val,h_val,s1_val] = EvaluateClassifier(valX, Wstar, bstar);

